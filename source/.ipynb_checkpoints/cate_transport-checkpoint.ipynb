{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the transportability of Conditional Average Treatment Effects \n",
    "\n",
    "Leo Guelman\n",
    "\n",
    "July 2022\n",
    "\n",
    "**Best current thinking**:\n",
    "\n",
    "* How to learn CATE models that can generalize?\n",
    "* Scenario 1 (Transportability): Access to data in target domain (Domain adapatation)\n",
    "* Scenario 2 (OOD Generalization): No access to data in target domain. Not having access to target domain data has two impication: 1) I cannot use data from target domain to detect changes in causal mechanisms; we need to inkove expert knowledge, 2) the only hope to get transportability is through direct transportability (as trivial transportability is not feasible)\n",
    "* Scenario 3 (Selection bias)\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* **Causal mechanism changes**\n",
    "* Example 5 - numerical example\n",
    "* Selection Bias (decided not to include from now, since problem is very similar to transportability and solution is exactly the same. It could make sense to potentially include distinction w.r.t. the synthatic differences). \n",
    "* Re-create general receipe including selection bias \n",
    "* Causal discovery with experimental data \n",
    "* End-to-end example\n",
    "* Refactor notebook / code\n",
    "* Slides\n",
    "* Think of next steps / further work (what would be a natural extension to these ideas). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many problems in statistics and causal inference can be framed as problems of generalizability. Consider as examples:\n",
    "\n",
    "1. **Classical statistical inference** is the process of generalizing properties from sampled data drawn from a population to properties of the population itself. https://link.springer.com/book/10.1007/978-0-387-46409-1\n",
    "\n",
    "2. **Statistical Transportability** (a.k.a. *dataset shift*, *domain adaptation*) departs from the traditional i.i.d. setting by allowing changes in distributions between training and testing and seeks to produce 'optimal' out-of-distribution generalizations in the absence of examples from the test task. [provide references, eg. Rojas]\n",
    "\n",
    "3. **Selection Bias** is a problem related to statistical transportability, but with important subtle differences. Here the problem is to infer statistical properties of a population from samples that have been selected preferentially, and where little is known about the underlying selection mechanism. [Heckman, ref]\n",
    "\n",
    "4. **Causal Inference** with observational data is the problem of generalizing causal-effect estimates from an observational regime to an experimental regime.[ref]\n",
    "\n",
    "5. **Causal Transportability** is the problem of inferring causal effects learned from experimental data to a new population in which only observational data are available. [ref]\n",
    "\n",
    "6. **Counterfactual Transportability** focuses on generalizing counterfactual distributions. This differs from statistical and causal transportability, which focuses on generalizing associational and interventional distributions. More specifically, causal transportability focuses on generalizing individual treatment effects (ITE) learned from experimental data to a new population in which only observational data are available. [ref]\n",
    "\n",
    "\n",
    "Here we focus on Causal Transportability of the Conditional Average Treatment Effect (CATE), which has received little attention in the literature. \n",
    "\n",
    "With the advent of \"personalization\" in medicine and business, there have a swell in methods for estimating the CATE [references]. In practice, these CATE estimators produce estimates based on experimental data (e.g., from a randomized controlled trial). More often than not, the goal is to generalize empirical findings to new environments, settings or populations. For instance, in the context of personalized pricing, we might have estimated the effect of a price discount on demand as a function of consumer characteristics in City 1 from experimental data. Under what conditions are we licensed to transfer these estimates to City 2, where only observational data are available? Similarly, in a clinical trial setting, we might have learned estimates of the effect of drug treatment on a patient's clinical response as a function of patient characteristics in Hospital 1. Again, under what conditions can we transport this knowledge to Hospital 2, where we only have access to the observational distribution. \n",
    "\n",
    "The post aims to introduce readers to the formal conditions under which CATEs in a target population can be inferred from experimental findings in the source population and provide practical guidance to Data Scientists for identifying and overcoming transportability issues in the CATE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Average Treatment Effect (CATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $T$ be a binary random variable representing treatment assignment and $Y$ be a random variable representing the observed outcome. Let $W=(X, Z)$ be a vector of covariates, where $X$ represents observed covariates and $Z$ the unobserved covariates. \n",
    "\n",
    "Using the Neyman/Rubin Potential Outcome notation (see Imbens \\& Rubin, 2015), the CATE $\\tau(x)$ is defined as the following estimand:\n",
    "\n",
    "$$\n",
    "\\tau(x) \\triangleq \\mathbb{E}[Y_i(1) - Y_i(0) | X = x],\n",
    "$$\n",
    "\n",
    "where $Y_i(t)$ is the potential outcome of unit $i=\\{1, \\ldots, N\\}$ when exposed to treatment $T=t, \\forall t \\in \\{0,1\\}$.\n",
    "\n",
    "Alternatively, using Pearl's *do*-operator, we can equivalently define the CATE as\n",
    "\n",
    "$$\n",
    "\\tau(x) \\triangleq \\mathbb{E}[Y_i|do(T=1), X=x] - \\mathbb{E}[Y_i|do(T=0), X=x]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There is often confusion in the literature between the CATE and a different estimand known as the Individual Treatment Effect (ITE), which is defined as\n",
    "\n",
    "$$\n",
    "\\tau_i \\triangleq \\mathbb{E}[Y_i|do(T=1)] - \\mathbb{E}[Y_i|do(T=0)].\n",
    "$$\n",
    "\n",
    "CATE and ITE are not the same, even under Ignorability assumptions (see Vegetabile, 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transportability Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a *probabilistic causal model* (PCM) defined as a pair $\\Psi := \\langle G, P_X \\rangle$ consisting of a causal graph $G$, and a joint distribution $P_X$ over the variables in $G$ that is compatible with $G$. Given a collection of $p$ random variables $X := (X_1, \\ldots, X_p)$, the underlying causal graph is a directed acyclic graph (DAG) in which a directed edge $X_i$ to $X_j$ indicates that $X_i$ is a direct cause of $X_j$. A joint distribution $P_X$ is compatible with $G$ if it satisfies the *causal Markov assumption*, which states that given its parents in the DAG, a node $X$ is independent of all its non-descendants. Hence, $P_X$ is compatible with $G$ if \n",
    "\n",
    "$$\n",
    "P_X = \\prod_{j=1}^{p}  P_{X_j} | \\text{pa}_j,\n",
    "$$\n",
    "\n",
    "where $P_{X_j} | \\text{pa}_j$ represents the *causal mechanism* for $X_j$, and we assume it remains invariant to interventions (external influences) in variables other than $X_j$. This assumption is known as *modularity*.\n",
    "\n",
    "**Note: Need to be more specific in terms of what is the problem statment. Is the problem to determine whether $P(Y=y|do(T=t), X=x)$ in the target population $\\pi^*$ is identifyable from the data at hand? What is precisely the question I'm trying to answer.** **See comments in example 5**.\n",
    "\n",
    "The transportability problem is as follows. Given two different populations $\\pi$ and $\\pi^*$, the goal is to estimate the conditional average treatment effect $P(Y=y|do(T=t), X=x)$ in the target population $\\pi^*$, based on experimental studies conducted on the source population $\\pi$. Concretely, we have access to the following information to estimate the CATE in the target population. First, the experimental findings from $\\pi$, $P(y|do(T=t),x)$. Second, the observational information from both $\\pi$ and $\\pi^*$,  $P(y,t,x)$ and $P^*(y,t,x)$, respectively. In addition, we assume access to the underlying causal graph $G$, which we assume remains the same between $\\pi$ and $\\pi^*$. However, causal mechanisms $P_{X_j} | \\text{pa}_j$ are allowed to change between populations. \n",
    "\n",
    "More formally, *changes in causal mechanisms* to a PCM  $\\Psi := \\langle G, P_X \\rangle$ on a subset of variables $X_S$ indexed by a changes set $S \\subseteq~\\{1, \\ldots, p\\}$ transform $\\Psi$ into $\\Psi^* := \\langle G, P^{S}_{X} \\rangle$, where \n",
    "\n",
    "$$\n",
    "P^{S}_{X} = \\prod_{j \\in S} \\tilde{P}_{X_j|\\text{pa}_j}\\prod_{j \\notin S} P_{X_j|\\text{pa}_j}\n",
    "$$\n",
    "\n",
    "is a new joint distribution obtained by replacing the \"old\" causal mechanism $P_{X_j} | \\text{pa}_j$ at each node $X_j$, where $j \\in S$, with a \"new\" causal mechanism $\\tilde{P}_{X_j} | \\text{pa}_j$.\n",
    "\n",
    "* Section 1 (Transportability and Selection Bias) - Pearl, Generalizing Experimental Finings\n",
    "* Definition 2 (Causal mechanism changes), need to relate this to selection diagrams - Vegetaible \n",
    "* G remains fix, P_X changes\n",
    "* Add selection node to reflect causal mechanism changes between $\\pi$ and $\\pi^*$\n",
    "* NOte changes in causal mechanisms, imply that either f changes or P(U) changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Causal DAG in Figure 1 as the working example. This graph represents the pre-treatment causal-effect relationships in city 1 (source population $\\pi$). We will use slight variations of this graph to determine transportability of the CATE $P(y|do(t), x)$ from city 1 to $P^*(y|do(t), x)$ in city 2, where $x$ represents a subset of the two features $x \\subseteq (\\text{Income, Savings})$ depending on the example. The bidirected dashed edges represent the presence of latent variables affecting the nodes they point. \n",
    "\n",
    "<img src=\"img/fig1.png\" \n",
    "     width=\"300\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: The two populations differ in the causal mechanism on Savings\n",
    "\n",
    "In this first example, we assume the two populations $\\pi$ and $\\pi^*$ only differ in the causal mechanism on Savings, $P(\\text{Savings}|\\text{Income})$ - i.e., in how Savings depends on Income. This is denoted by the blue $\\delta$-node with an edge pointing to $\\text{Savings}$. Also, we assume Income is not observed. \n",
    "\n",
    "<img src=\"img/fig2.png\" \n",
    "     width=\"250\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >\n",
    "\n",
    "We now proceed as follows:\n",
    "\n",
    "* Generate sample according to the DGP in Figure 2 in source $\\pi$ and target $\\pi^*$ populations. We have:\n",
    "\n",
    "    1. The experimental data from $\\pi$, $P(y|do(T=t),x)$. \n",
    "    2. The observational information from both $\\pi$ and $\\pi^*$,  $P(y,t,x)$ and $P^*(y,t,x)$\n",
    "    \n",
    "* We fit a model to estimate the following CATE model $P(y | do(t), \\text{Savings})$ to source population $\\pi$ (assume we don't have access to causal graph when fitting the model)\n",
    "* Actual vs predicted CATE on source population (out of sample, in-distribution) vs target population (out of sample, out of distribution). This model should do well in source but very badly in target population.\n",
    "* Adjust model based on transport forumla for $P^*(y | do(t), \\text{Savings})$ in Pearl (right side of Page 585), and show performance (this is the right trasnport method assuming access to the DAG)\n",
    "\n",
    "\n",
    "Suppose the causal DAG in Figure 2 is induced by the following Structural Causal Model:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_1 & : = N_{U_1} \\\\\n",
    "U_2 & : = N_{U_2} \\\\\n",
    "\\text{Income} & := 0.5U_1 + N_{\\text{Income}} \\\\\n",
    "\\text{Savings} & := 0.5\\text{Income} + 0.1 N_{\\text{Savings}} \\\\\n",
    "T & := 0.2U_1 + 0.2U_2 + 1.5\\text{Income} + 0.3 N_{T} \\\\\n",
    "Y & := 0.8\\text{Income} + 1.5T + 2 \\text{Income} \\times T + 0.2 * U_2 + 0.4 N_{Y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the noise terms are independent $N_j \\sim \\mathcal{N}(0,1)$. Also consider:\n",
    "\n",
    "* A randomized trial in the source population which sets $T ~\\sim \\text{Bernoulli}(0.5)$. This transforms the graph in Figure 2 to a new graph in which all causal parents of T have been removed (i.e., all incoming edges are removed as T depends now on a coin toss).\n",
    "* The SCM in city 2 is the same as above, except that the causal mechanism $P(\\text{Savings}|\\text{Income})$ changes to: $\\text{Savings} := \\text{Income}^2 + 0.1 N_{\\text{Savings}}$, reflecting that in City 2 the savings rate is significantly lower than in City 1. \n",
    "\n",
    "**Note**: Notice this model is not directly transportable since Savings is a collider, and conditioning on it, opens a path between $\\delta$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def scm_example_1(n_samples:int=100, city=None, seed = None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N = np.random.normal(0.0, 1.0, (n_samples, 6))\n",
    "    N_U1, N_U2, N_I, N_S, N_T, N_Y =  (N[:,0], N[:,1], N[:,2], N[:,3], N[:,4], N[:,5])\n",
    "    U1 = N_U1\n",
    "    U2 = N_U2\n",
    "    I = 0.5 * U1 +  N_I\n",
    "    \n",
    "    if city==1:\n",
    "        S = 0.5 * I + 0.1 * N_S\n",
    "        T = np.random.binomial(1, 0.5, n_samples)\n",
    "        \n",
    "    elif city ==2:\n",
    "        S = I ** 2 + 0.1 * N_S\n",
    "        T = 0.2 * U1 + 0.2 * U2 + 1.5 * I + 0.3 * N_T\n",
    "    \n",
    "    Y_obs = 0.8 * I + 1.5 * T + 2 * I * T + 0.2 * U2 + 0.4 * N_Y\n",
    "    Y1 = 0.8 * I + 1.5 * 1 + 2 * I * 1 + 0.2 * U2 + 0.4 * N_Y\n",
    "    Y0 = 0.8 * I + 1.5 * 0 + 2 * I * 0 + 0.2 * U2 + 0.4 * N_Y\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'I':I, 'S':S, 'T':T, 'ST': S * T, 'IT': I * T, \n",
    "                                 'Y_obs':Y_obs, 'Y1':Y1, 'Y0':Y0})\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9822619357398423\n",
      "0.051247344947500745\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "a = scm_example_1(1000, 1, 42)\n",
    "print(stats.pearsonr(a['I'].values, a['S'].values)[0])\n",
    "\n",
    "b = scm_example_1(1000, 2, 82)\n",
    "print(stats.pearsonr(b['I'].values, b['S'].values)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>y_test_diff_pred</th>\n",
       "      <th>y_test_diff</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-3.0, -1.5]</th>\n",
       "      <td>-5.269194</td>\n",
       "      <td>-5.594017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.5, -1.0]</th>\n",
       "      <td>-2.881864</td>\n",
       "      <td>-2.890231</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.0, -0.5]</th>\n",
       "      <td>-1.094777</td>\n",
       "      <td>-1.108825</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.5, 0.0]</th>\n",
       "      <td>0.609001</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.5]</th>\n",
       "      <td>2.421508</td>\n",
       "      <td>2.426093</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 1.0]</th>\n",
       "      <td>4.322606</td>\n",
       "      <td>4.288114</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.0, 1.5]</th>\n",
       "      <td>6.191840</td>\n",
       "      <td>6.363516</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.5, 3.0]</th>\n",
       "      <td>7.590742</td>\n",
       "      <td>7.876372</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_test_diff_pred y_test_diff     S\n",
       "                         mean        mean count\n",
       "S_quantile                                     \n",
       "(-3.0, -1.5]        -5.269194   -5.594017     2\n",
       "(-1.5, -1.0]        -2.881864   -2.890231    22\n",
       "(-1.0, -0.5]        -1.094777   -1.108825   141\n",
       "(-0.5, 0.0]          0.609001    0.547348   336\n",
       "(0.0, 0.5]           2.421508    2.426093   341\n",
       "(0.5, 1.0]           4.322606    4.288114   126\n",
       "(1.0, 1.5]           6.191840    6.363516    30\n",
       "(1.5, 3.0]           7.590742    7.876372     2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample\n",
    "\n",
    "data_city1 = scm_example_1(2000, 1, 42)\n",
    "\n",
    "train_city1, test_city1 = train_test_split(data_city1, test_size=0.5)\n",
    "\n",
    "test_city1_T0 = test_city1.copy()\n",
    "test_city1_T0['T'] = 0\n",
    "test_city1_T0['ST'] = test_city1_T0['S'] * test_city1_T0['T']\n",
    "test_city1_T1 = test_city1.copy()\n",
    "test_city1_T1['T'] = 1\n",
    "test_city1_T1['ST'] = test_city1_T1['S'] * test_city1_T1['T']\n",
    "\n",
    "X_train = train_city1[['S', 'T', 'ST']].values\n",
    "y_train = train_city1[['Y_obs']].values\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "X_test_T0 = test_city1_T0[['S', 'T', 'ST']].values\n",
    "X_test_T1 = test_city1_T1[['S', 'T', 'ST']].values\n",
    "y_test_T0_pred = reg.predict(X_test_T0)\n",
    "y_test_T1_pred = reg.predict(X_test_T1)\n",
    "\n",
    "\n",
    "# Out-of-sample / In-distribution (city 1) predictions \n",
    "df_city1_res = pd.DataFrame.from_dict({'S':test_city1['S'].values, \n",
    "                                       'y_test_T0_pred': y_test_T0_pred.squeeze(),\n",
    "                                       'y_test_T1_pred': y_test_T1_pred.squeeze(),\n",
    "                                       'y_test_diff_pred': y_test_T1_pred.squeeze() - y_test_T0_pred.squeeze(),\n",
    "                                       'y_test_diff': test_city1['Y1'].values - test_city1['Y0'].values\n",
    "                                      })\n",
    "\n",
    "df_city1_res['S_quantile'] = pd.cut(df_city1_res['S'], [-3,  -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 3], labels=None)\n",
    "\n",
    "print(df_city1_res.groupby(['S_quantile']).agg({'y_test_diff_pred':['mean'], 'y_test_diff': ['mean'], 'S':['count'] })['S'].sum()==1000)\n",
    "\n",
    "df_city1_res.groupby(['S_quantile']).agg({'y_test_diff_pred':['mean'], 'y_test_diff': ['mean'], 'S':['count'] })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this uplift model produces valid out-of-sample estimates in population $\\pi$ (in-distribution). However, as shown below, it fails badly in population $\\pi^*$ (out-of-distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>y_city2_diff_pred</th>\n",
       "      <th>y_city2_diff</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-20.0, -1.5]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.5, -1.0]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.0, -0.5]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.5, 0.0]</th>\n",
       "      <td>1.282522</td>\n",
       "      <td>1.509443</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.5]</th>\n",
       "      <td>2.315016</td>\n",
       "      <td>1.519031</td>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 1.0]</th>\n",
       "      <td>4.297512</td>\n",
       "      <td>1.478514</td>\n",
       "      <td>1571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.0, 1.5]</th>\n",
       "      <td>6.209791</td>\n",
       "      <td>1.488005</td>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.5, 20.0]</th>\n",
       "      <td>14.699071</td>\n",
       "      <td>1.511756</td>\n",
       "      <td>2810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              y_city2_diff_pred y_city2_diff     S\n",
       "                           mean         mean count\n",
       "S_quantile                                        \n",
       "(-20.0, -1.5]               NaN          NaN     0\n",
       "(-1.5, -1.0]                NaN          NaN     0\n",
       "(-1.0, -0.5]                NaN          NaN     0\n",
       "(-0.5, 0.0]            1.282522     1.509443   905\n",
       "(0.0, 0.5]             2.315016     1.519031  3760\n",
       "(0.5, 1.0]             4.297512     1.478514  1571\n",
       "(1.0, 1.5]             6.209791     1.488005   954\n",
       "(1.5, 20.0]           14.699071     1.511756  2810"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out-of-sample / Out-of-distribution (city 2) predictions \n",
    "data_city2 = scm_example_1(10000, 2, 83)\n",
    "\n",
    "city2_T0 = data_city2.copy()\n",
    "city2_T0['T'] = 0\n",
    "city2_T0['ST'] = city2_T0['S'] * city2_T0['T']\n",
    "city2_T1 = data_city2.copy()\n",
    "city2_T1['T'] = 1\n",
    "city2_T1['ST'] = city2_T1['S'] * city2_T1['T']\n",
    "\n",
    "\n",
    "X_city2_T0 = city2_T0[['S', 'T', 'ST']].values\n",
    "X_city2_T1 = city2_T1[['S', 'T', 'ST']].values\n",
    "y_city2_T0_pred = reg.predict(X_city2_T0)\n",
    "y_city2_T1_pred = reg.predict(X_city2_T1)\n",
    "\n",
    "df_city2_res = pd.DataFrame.from_dict({'S':data_city2['S'].values, \n",
    "                                       'y_city2_T0_pred': y_city2_T0_pred.squeeze(),\n",
    "                                       'y_city2_T1_pred': y_city2_T1_pred.squeeze(),\n",
    "                                       'y_city2_diff_pred': y_city2_T1_pred.squeeze() - y_city2_T0_pred.squeeze(),\n",
    "                                       'y_city2_diff': data_city2['Y1'].values - data_city2['Y0'].values\n",
    "                                      })\n",
    "\n",
    "df_city2_res['S_quantile'] = pd.cut(df_city2_res['S'], [-20,  -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 20], labels=None)\n",
    "\n",
    "print(df_city2_res.groupby(['S_quantile']).agg({'y_city2_diff_pred':['mean'], 'y_city2_diff': ['mean'], 'S':['count'] })['S'].sum()==10000)\n",
    "\n",
    "df_city2_res.groupby(['S_quantile']).agg({'y_city2_diff_pred':['mean'], 'y_city2_diff': ['mean'], 'S':['count'] })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's derive the right transport formula for the CATE $P^*(y | do(t), \\text{Savings})$\n",
    "\n",
    "\\begin{align}\n",
    "&P^*(y | do(t), S) \\\\\n",
    "&= \\sum_{I} P^*(y | do(t), S, I) P^*(I|do(t), S) \\\\\n",
    "&= \\sum_{I} P^*(y | do(t), I) P^*(I|S) \\\\\n",
    "&= \\sum_{I} P(y | do(t), I) P^*(I|S)\n",
    "\\end{align}\n",
    "\n",
    "To verify that this adjustment formula produces valid CATE estimates in population $\\pi^*$, let's assume we have access to income data in population $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>y_city2_diff_pred</th>\n",
       "      <th>y_city2_diff</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-20.0, -1.5]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.5, -1.0]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.0, -0.5]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.5, 0.0]</th>\n",
       "      <td>1.530116</td>\n",
       "      <td>1.509443</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.5]</th>\n",
       "      <td>1.539583</td>\n",
       "      <td>1.519031</td>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 1.0]</th>\n",
       "      <td>1.499576</td>\n",
       "      <td>1.478514</td>\n",
       "      <td>1571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.0, 1.5]</th>\n",
       "      <td>1.508948</td>\n",
       "      <td>1.488005</td>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.5, 20.0]</th>\n",
       "      <td>1.532401</td>\n",
       "      <td>1.511756</td>\n",
       "      <td>2810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              y_city2_diff_pred y_city2_diff     I\n",
       "                           mean         mean count\n",
       "S_quantile                                        \n",
       "(-20.0, -1.5]               NaN          NaN     0\n",
       "(-1.5, -1.0]                NaN          NaN     0\n",
       "(-1.0, -0.5]                NaN          NaN     0\n",
       "(-0.5, 0.0]            1.530116     1.509443   905\n",
       "(0.0, 0.5]             1.539583     1.519031  3760\n",
       "(0.5, 1.0]             1.499576     1.478514  1571\n",
       "(1.0, 1.5]             1.508948     1.488005   954\n",
       "(1.5, 20.0]            1.532401     1.511756  2810"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_city1[['I', 'T', 'IT']].values\n",
    "y_train = train_city1[['Y_obs']].values\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "city2_T0 = data_city2.copy()\n",
    "city2_T0['T'] = 0\n",
    "city2_T0['IT'] = city2_T0['I'] * city2_T0['T']\n",
    "city2_T1 = data_city2.copy()\n",
    "city2_T1['T'] = 1\n",
    "city2_T1['IT'] = city2_T1['I'] * city2_T1['T']\n",
    "\n",
    "X_city2_T0 = city2_T0[['I', 'T', 'IT']].values\n",
    "X_city2_T1 = city2_T1[['I', 'T', 'IT']].values\n",
    "y_city2_T0_pred = reg.predict(X_city2_T0)\n",
    "y_city2_T1_pred = reg.predict(X_city2_T1)\n",
    "y_city2_diff_pred =  y_city2_T1_pred.squeeze() - y_city2_T0_pred.squeeze()\n",
    "S_quantile = pd.cut(data_city2['S'].values, [-20,  -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 20], labels=None)\n",
    "\n",
    "df_city2_res = pd.DataFrame.from_dict({'S_quantile':S_quantile, \n",
    "                                       'I': data_city2['I'].values,\n",
    "                                       'y_city2_diff_pred': y_city2_diff_pred,\n",
    "                                       'y_city2_diff': data_city2['Y1'].values - data_city2['Y0'].values\n",
    "                                      })\n",
    "\n",
    "df_city2_res.groupby(['S_quantile']).agg({'y_city2_diff_pred':['mean'], 'y_city2_diff': ['mean'], 'I':['count'] })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: The two populations differ in the causal mechanism on Income\n",
    "\n",
    "In this example, we assume the two populations $\\pi$ and $\\pi^*$ only differ in the causal mechanism on Income, $P(\\text{Income})$ - i.e., the populations differ in the income distributions. This is denoted by the blue $\\delta$-node with an edge pointing to $\\text{Income}$. \n",
    "\n",
    "\n",
    "<img src=\"img/fig3.png\" \n",
    "     width=\"250\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >\n",
    "\n",
    "As we assume that the only difference between the two populations is attributed to a $\\delta$ variable responsible for shifting the income distribution in the target population, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P^*(y|do(t), \\text{Income}) & = P(y|do(t), \\text{Income}, \\delta)\\\\\n",
    "& = P(y|do(t), \\text{Income}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last equality follows from Rule 1 of *do*-calculus whenever Income satisfies $(Y \\perp \\!\\!\\! \\perp \\delta | T, \\text{Income})$ in $G_{\\bar{T}}$, where $G_{\\bar{T}}$ represents the graph with all incoming edges pointing to $T$ are removed (which holds here due to randomization of $T$ in population $\\pi$). Hence, in this scenario we have a case of *direct transportability* (DT) of CATE. The CATE is directly transportable if $P^*(y|do(t), x) = P(y|do(t), x)$. In fact, a general graphical condition for *direct transportability* (DT) of CATE is given by ($Y \\perp \\!\\!\\! \\perp \\delta | T, X$) in $G_{\\bar{T}}$ - i.e., $Y$ is independent of $\\delta$ after we condition on $X$ and $T$ in the graph where all incoming edges to $T$ are removed (Pearl and Barenboim). \n",
    "\n",
    "\n",
    "\n",
    "Let's verify that in this case we have DT through a simple simulation using the following Structural Causal Model:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_1 & : = N_{U_1} \\\\\n",
    "U_2 & : = N_{U_2} \\\\\n",
    "\\text{Income} & := 0.5U_1 + N_{\\text{Income}} \\\\\n",
    "T & := 0.2U_1 + 0.2U_2 + 1.5\\text{Income} + 0.3 N_{T} \\\\\n",
    "Y & := 0.8\\text{Income} + 1.5T + 2 \\text{Income} \\times T + 0.2 * U_2 + 0.4 N_{Y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the noise terms are independent $N_j \\sim \\mathcal{N}(0,1)$. Also consider:\n",
    "\n",
    "* A randomized trial in the source population which sets $T ~\\sim \\text{Bernoulli}(0.5)$. \n",
    "\n",
    "* The SCM in city 2 is the same as above, except that the causal mechanism $P(\\text{Income})$ changes to: $\\text{Income} := U_1^2 + 0.1 N_{\\text{Income}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scm_example_2(n_samples:int=100, city=None, seed = None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N = np.random.normal(0.0, 1.0, (n_samples, 5))\n",
    "    N_U1, N_U2, N_I, N_T, N_Y =  (N[:,0], N[:,1], N[:,2], N[:,3], N[:,4])\n",
    "    U1 = N_U1\n",
    "    U2 = N_U2\n",
    "    \n",
    "    if city==1:\n",
    "        I = 0.5 * U1 +  N_I\n",
    "        T = np.random.binomial(1, 0.5, n_samples)\n",
    "        T_ = 0.2 * U1 + 0.2 * U2 + 1.5 * I + 0.3 * N_T\n",
    "        \n",
    "    elif city ==2:\n",
    "        I = U1**2 + 0.1 * N_I\n",
    "        T = 0.2 * U1 + 0.2 * U2 + 1.5 * I + 0.3 * N_T\n",
    "        T_ = None\n",
    "    \n",
    "    Y_obs = 0.8 * I + 1.5 * T + 2 * I * T + 0.2 * U2 + 0.4 * N_Y\n",
    "    Y1 = 0.8 * I + 1.5 * 1 + 2 * I * 1 + 0.2 * U2 + 0.4 * N_Y\n",
    "    Y0 = 0.8 * I + 1.5 * 0 + 2 * I * 0 + 0.2 * U2 + 0.4 * N_Y\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'I':I, 'T':T, 'IT': I * T, \n",
    "                                 'Y_obs':Y_obs, 'Y1':Y1, 'Y0':Y0, 'T_':T_})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>y_test_diff_pred</th>\n",
       "      <th>y_test_diff</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I_quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-20.0, -3.0]</th>\n",
       "      <td>-4.799532</td>\n",
       "      <td>-4.768907</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-3.0, -1.5]</th>\n",
       "      <td>-2.412367</td>\n",
       "      <td>-2.378312</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.5, -1.0]</th>\n",
       "      <td>-1.022962</td>\n",
       "      <td>-0.986910</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.0, -0.5]</th>\n",
       "      <td>-0.028897</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.5, 0.0]</th>\n",
       "      <td>0.973221</td>\n",
       "      <td>1.012140</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.5]</th>\n",
       "      <td>1.937446</td>\n",
       "      <td>1.977750</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 1.0]</th>\n",
       "      <td>2.951324</td>\n",
       "      <td>2.993085</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.0, 1.5]</th>\n",
       "      <td>3.915192</td>\n",
       "      <td>3.958339</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.5, 3.0]</th>\n",
       "      <td>5.466451</td>\n",
       "      <td>5.511825</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3.0, 20.0]</th>\n",
       "      <td>8.257458</td>\n",
       "      <td>8.306843</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              y_test_diff_pred y_test_diff     I\n",
       "                          mean        mean count\n",
       "I_quantile                                      \n",
       "(-20.0, -3.0]        -4.799532   -4.768907     2\n",
       "(-3.0, -1.5]         -2.412367   -2.378312    81\n",
       "(-1.5, -1.0]         -1.022962   -0.986910    96\n",
       "(-1.0, -0.5]         -0.028897    0.008582   136\n",
       "(-0.5, 0.0]           0.973221    1.012140   170\n",
       "(0.0, 0.5]            1.937446    1.977750   190\n",
       "(0.5, 1.0]            2.951324    2.993085   147\n",
       "(1.0, 1.5]            3.915192    3.958339    88\n",
       "(1.5, 3.0]            5.466451    5.511825    86\n",
       "(3.0, 20.0]           8.257458    8.306843     4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample\n",
    "\n",
    "data_city1 = scm_example_2(2000, 1, 42)\n",
    "\n",
    "train_city1, test_city1 = train_test_split(data_city1, test_size=0.5)\n",
    "\n",
    "test_city1_T0 = test_city1.copy()\n",
    "test_city1_T0['T'] = 0\n",
    "test_city1_T0['IT'] = test_city1_T0['I'] * test_city1_T0['T']\n",
    "test_city1_T1 = test_city1.copy()\n",
    "test_city1_T1['T'] = 1\n",
    "test_city1_T1['IT'] = test_city1_T1['I'] * test_city1_T1['T']\n",
    "\n",
    "X_train = train_city1[['I', 'T', 'IT']].values\n",
    "y_train = train_city1[['Y_obs']].values\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "X_test_T0 = test_city1_T0[['I', 'T', 'IT']].values\n",
    "X_test_T1 = test_city1_T1[['I', 'T', 'IT']].values\n",
    "y_test_T0_pred = reg.predict(X_test_T0)\n",
    "y_test_T1_pred = reg.predict(X_test_T1)\n",
    "\n",
    "\n",
    "# Out-of-sample / In-distribution (city 1) predictions \n",
    "df_city1_res = pd.DataFrame.from_dict({'I':test_city1['I'].values, \n",
    "                                       'y_test_T0_pred': y_test_T0_pred.squeeze(),\n",
    "                                       'y_test_T1_pred': y_test_T1_pred.squeeze(),\n",
    "                                       'y_test_diff_pred': y_test_T1_pred.squeeze() - y_test_T0_pred.squeeze(),\n",
    "                                       'y_test_diff': test_city1['Y1'].values - test_city1['Y0'].values\n",
    "                                      })\n",
    "\n",
    "df_city1_res['I_quantile'] = pd.cut(df_city1_res['I'], [-20, -3,  -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 3, 20], labels=None)\n",
    "\n",
    "print(df_city1_res.groupby(['I_quantile']).agg({'y_test_diff_pred':['mean'], 'y_test_diff': ['mean'], 'I':['count'] })['I'].sum()==1000)\n",
    "\n",
    "df_city1_res.groupby(['I_quantile']).agg({'y_test_diff_pred':['mean'], 'y_test_diff': ['mean'], 'I':['count'] })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>y_city2_diff_pred</th>\n",
       "      <th>y_city2_diff</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I_quantile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-20.0, -3.0]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-3.0, -1.5]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.5, -1.0]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.0, -0.5]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.5, 0.0]</th>\n",
       "      <td>1.325157</td>\n",
       "      <td>1.364582</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.5]</th>\n",
       "      <td>1.864254</td>\n",
       "      <td>1.904453</td>\n",
       "      <td>4173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 1.0]</th>\n",
       "      <td>2.907966</td>\n",
       "      <td>2.949665</td>\n",
       "      <td>1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.0, 1.5]</th>\n",
       "      <td>3.926523</td>\n",
       "      <td>3.969685</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.5, 3.0]</th>\n",
       "      <td>5.705936</td>\n",
       "      <td>5.751655</td>\n",
       "      <td>1403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3.0, 20.0]</th>\n",
       "      <td>11.037257</td>\n",
       "      <td>11.090636</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              y_city2_diff_pred y_city2_diff     I\n",
       "                           mean         mean count\n",
       "I_quantile                                        \n",
       "(-20.0, -3.0]               NaN          NaN     0\n",
       "(-3.0, -1.5]                NaN          NaN     0\n",
       "(-1.5, -1.0]                NaN          NaN     0\n",
       "(-1.0, -0.5]                NaN          NaN     0\n",
       "(-0.5, 0.0]            1.325157     1.364582  1020\n",
       "(0.0, 0.5]             1.864254     1.904453  4173\n",
       "(0.5, 1.0]             2.907966     2.949665  1633\n",
       "(1.0, 1.5]             3.926523     3.969685   927\n",
       "(1.5, 3.0]             5.705936     5.751655  1403\n",
       "(3.0, 20.0]           11.037257    11.090636   844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out-of-sample / Out-of-distribution (city 2) predictions \n",
    "data_city2 = scm_example_2(10000, 2, 83)\n",
    "\n",
    "city2_T0 = data_city2.copy()\n",
    "city2_T0['T'] = 0\n",
    "city2_T0['IT'] = city2_T0['I'] * city2_T0['T']\n",
    "city2_T1 = data_city2.copy()\n",
    "city2_T1['T'] = 1\n",
    "city2_T1['IT'] = city2_T1['I'] * city2_T1['T']\n",
    "\n",
    "\n",
    "X_city2_T0 = city2_T0[['I', 'T', 'IT']].values\n",
    "X_city2_T1 = city2_T1[['I', 'T', 'IT']].values\n",
    "y_city2_T0_pred = reg.predict(X_city2_T0)\n",
    "y_city2_T1_pred = reg.predict(X_city2_T1)\n",
    "\n",
    "df_city2_res = pd.DataFrame.from_dict({'I':data_city2['I'].values, \n",
    "                                       'y_city2_T0_pred': y_city2_T0_pred.squeeze(),\n",
    "                                       'y_city2_T1_pred': y_city2_T1_pred.squeeze(),\n",
    "                                       'y_city2_diff_pred': y_city2_T1_pred.squeeze() - y_city2_T0_pred.squeeze(),\n",
    "                                       'y_city2_diff': data_city2['Y1'].values - data_city2['Y0'].values\n",
    "                                      })\n",
    "\n",
    "df_city2_res['I_quantile'] = pd.cut(df_city2_res['I'], [-20, -3,  -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 3, 20], labels=None)\n",
    "\n",
    "print(df_city2_res.groupby(['I_quantile']).agg({'y_city2_diff_pred':['mean'], 'y_city2_diff': ['mean'], 'I':['count'] })['I'].sum()==10000)\n",
    "\n",
    "df_city2_res.groupby(['I_quantile']).agg({'y_city2_diff_pred':['mean'], 'y_city2_diff': ['mean'], 'I':['count'] })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: The two populations differ in the causal mechanism on Y\n",
    "\n",
    "Suppose the two populations $\\pi$ and $\\pi^*$ differ in the causal mechanism for $Y$, as in Figure 4. In this scenario the graphical condition for direct transportability does not hold - i.e., ($Y \\not\\perp \\!\\!\\!\\!\\! \\perp \\delta | T, \\text{Income}$) in $G_{\\bar{T}}$. However, although the CATE is not directly transportable in this scenario, it can be trivially estimated from observational data $P^*(y,t,x)$ in population $\\pi^*$. More specifically, $P^*(y|do(t), \\text{Income}) = P^*(y|t, \\text{Income}) $.\n",
    "\n",
    "<img src=\"img/fig4.png\" \n",
    "     width=\"250\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "\n",
    "Suppose the data generating process now follows the DAG in Figure 5. This is sort of similar to Example 1 with one additional node $\\text{Age}$ pointing to $Y$. In this example though we will shows how conventional CATE model selection methods [ref shuller] will select a model that will do better in distribution, but will fail to generalize out of distribution. \n",
    "\n",
    "Alejandro Schuler, Michael Baiocchi, Robert Tibshirani, Nigam Shah. “A comparison of methods for model selection when estimating individual treatment effects.” Arxiv, 2018 https://arxiv.org/pdf/1804.05146.pdf\n",
    "\n",
    "More specifically, suppose this DAG is induced by the following Structural Causal Model:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_1 & : = N_{U_1} \\\\\n",
    "U_2 & : = N_{U_2} \\\\\n",
    "\\text{Income} & := 0.5U_1 + N_{\\text{Income}} \\\\\n",
    "\\text{Savings} & := 0.5\\text{Income} + 0.1 N_{\\text{Savings}} \\\\\n",
    "\\text{Age} & := N_{\\text{Age}} \\\\\n",
    "T & := 0.2U_1 + 0.2U_2 + 1.5X_1 + 0.3 N_{T} \\\\\n",
    "Y & := 1.5T + 0.8\\text{Income} + 2 \\text{Income} \\times T + 0.5 \\text{Age} + 0.5 \\text{Age} \\times T  + 0.2U_2 + 0.4 N_{Y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the noise terms are independent $N_j \\sim \\mathcal{N}(0,1)$. Also consider:\n",
    "\n",
    "* A randomized trial in the source population which sets $T ~\\sim \\text{Bernoulli}(0.5)$. This transforms the graph in Figure 5 to a new graph in which all causal parents of T have been removed (i.e., all incoming edges are removed as T depends now on a coin toss).\n",
    "* The SCM in city 2 is the same as above, except that the causal mechanism $P(\\text{Savings}|\\text{Income})$ changes to: $\\text{Savings} := \\text{Income}^2 + 0.1 N_{\\text{Savings}}$, reflecting that in City 2 the savings rate is significantly lower than in City 1.\n",
    "\n",
    "Now, consider following two alternative CATE model specifications:\n",
    "\n",
    "Model 1: $P(y|do(t), \\text{Savings}, \\text{Age})$ \n",
    "\n",
    "Model 2: $P(y|do(t), \\text{Age})$\n",
    "\n",
    "We will show that model 1 does better out of sample but in-distribution than model 2, whereas the opposite will hold out of distribution. Since $X_2$ is a collider, conditioning on $X_2$ induce dependence between $Y$ and $\\delta$, and hence produces a model that fails to generalize to the new environment.  \n",
    "\n",
    "<img src=\"img/fig6.png\" \n",
    "     width=\"250\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >\n",
    "     \n",
    "Note in this case model 2 is directly transportable since Y is conditionally independent of delta conditioned on Age on the graph where all incoming nodes to T have been removed. Note that Savings is a collider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def scm_example_4(n_samples:int=100, city=None, seed = None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N = np.random.normal(0.0, 1.0, (n_samples, 7))\n",
    "    N_U1, N_U2, N_I, N_A, N_S, N_T, N_Y =  (N[:,0], N[:,1], N[:,2], N[:,3], N[:,4], N[:,5],  N[:,6])\n",
    "    U1 = N_U1\n",
    "    U2 = N_U2\n",
    "    A = N_A\n",
    "    I = 0.5 * U1 +  N_I\n",
    "    \n",
    "    if city==1:\n",
    "        S = 0.5 * I + 0.1 * N_S\n",
    "        T = np.random.binomial(1, 0.5, n_samples)\n",
    "        \n",
    "    elif city ==2:\n",
    "        S = I ** 2 + 0.1 * N_S\n",
    "        T = 0.2 * U1 + 0.2 * U2 + 1.5 * I + 0.3 * N_T\n",
    "    \n",
    "    Y_obs = 1.5 * T + 0.8 * I + 2 * I * T + 0.5 * A + 0.5 * A * T + 0.2 * U2 + 0.4 * N_Y\n",
    "    \n",
    "    Y1 = 1.5 * 1 + 0.8 * I + 2 * I * 1 + 0.5 * A + 0.5 * A * 1 + 0.2 * U2 + 0.4 * N_Y\n",
    "    Y0 = 1.5 * 0 + 0.8 * I + 2 * I * 0 + 0.5 * A + 0.5 * A * 0 + 0.2 * U2 + 0.4 * N_Y\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'I':I, 'S':S, 'A': A, 'T':T, 'ST': S * T, 'AT': A * T, \n",
    "                                 'Y_obs':Y_obs, 'Y1':Y1, 'Y0':Y0})\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample\n",
    "\n",
    "data_city1 = scm_example_4(2000, 1, 42)\n",
    "train_city1, test_city1 = train_test_split(data_city1, test_size=0.5)\n",
    "\n",
    "X_train1 = train_city1[['S', 'A', 'ST', 'AT']].values\n",
    "y_train1 = train_city1[['Y_obs']].values\n",
    "mod1 = LinearRegression().fit(X_train1, y_train1)\n",
    "\n",
    "X_train2 = train_city1[['A', 'AT']].values\n",
    "y_train2 = train_city1[['Y_obs']].values\n",
    "mod2 = LinearRegression().fit(X_train2, y_train2)\n",
    "\n",
    "test_city1_T0 = test_city1.copy()\n",
    "test_city1_T0['T'] = 0\n",
    "test_city1_T0['ST'] = test_city1_T0['S'] * test_city1_T0['T']\n",
    "test_city1_T0['AT'] = test_city1_T0['A'] * test_city1_T0['T']\n",
    "\n",
    "test_city1_T1 = test_city1.copy()\n",
    "test_city1_T1['T'] = 1\n",
    "test_city1_T1['ST'] = test_city1_T1['S'] * test_city1_T1['T']\n",
    "test_city1_T1['AT'] = test_city1_T1['A'] * test_city1_T1['T']\n",
    "\n",
    "X_test_T0 = test_city1_T0[['S', 'A', 'ST', 'AT']].values\n",
    "X_test_T1 = test_city1_T1[['S', 'A', 'ST', 'AT']].values\n",
    "y_test_T0_pred_mod1 = mod1.predict(X_test_T0)\n",
    "y_test_T1_pred_mod1 = mod1.predict(X_test_T1)\n",
    "\n",
    "X_test_T0 = test_city1_T0[['A', 'AT']].values\n",
    "X_test_T1 = test_city1_T1[['A', 'AT']].values\n",
    "y_test_T0_pred_mod2 = mod2.predict(X_test_T0)\n",
    "y_test_T1_pred_mod2 = mod2.predict(X_test_T1)\n",
    "\n",
    "# out of sample, In-distribution (city 1) predictions model 1\n",
    "df_city1_mod1 = pd.DataFrame.from_dict({'y_test_T0_pred_mod1': y_test_T0_pred_mod1.squeeze(),\n",
    "                                        'y_test_T1_pred_mod1': y_test_T1_pred_mod1.squeeze(),\n",
    "                                        'y_test_diff_pred_mod1': y_test_T1_pred_mod1.squeeze() - y_test_T0_pred_mod1.squeeze(),\n",
    "                                        'y_test_diff': test_city1['Y1'].values - test_city1['Y0'].values\n",
    "                                       })\n",
    "\n",
    "# out of sample, In-distribution (city 1) predictions model 2\n",
    "\n",
    "df_city1_mod2 = pd.DataFrame.from_dict({'y_test_T0_pred_mod2': y_test_T0_pred_mod2.squeeze(),\n",
    "                                        'y_test_T1_pred_mod2': y_test_T1_pred_mod2.squeeze(),\n",
    "                                        'y_test_diff_pred_mod2': y_test_T1_pred_mod2.squeeze() - y_test_T0_pred_mod2.squeeze(),\n",
    "                                        'y_test_diff': test_city1['Y1'].values - test_city1['Y0'].values\n",
    "                                        })\n",
    "\n",
    "\n",
    "\n",
    "print(\"MSE - Model 1 - In Distribution:\", \n",
    "mean_squared_error(df_city1_mod1['y_test_diff'].values, df_city1_mod1['y_test_diff_pred_mod1'].values))\n",
    "\n",
    "print(\"MSE - Model 2 - In Distribution:\", \n",
    "mean_squared_error(df_city1_mod2['y_test_diff'].values, df_city1_mod2['y_test_diff_pred_mod2'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-sample / Out-of-distribution (city 2) predictions \n",
    "data_city2 = scm_example_4(10000, 2, 83)\n",
    "\n",
    "test_city2_T0 = data_city2.copy()\n",
    "test_city2_T0['T'] = 0\n",
    "test_city2_T0['ST'] = test_city2_T0['S'] * test_city2_T0['T']\n",
    "test_city2_T0['AT'] = test_city2_T0['A'] * test_city2_T0['T']\n",
    "\n",
    "test_city2_T1 = data_city2.copy()\n",
    "test_city2_T1['T'] = 1\n",
    "test_city2_T1['ST'] = test_city2_T1['S'] * test_city2_T1['T']\n",
    "test_city2_T1['AT'] = test_city2_T1['A'] * test_city2_T1['T']\n",
    "\n",
    "X_test_T0 = test_city2_T0[['S', 'A', 'ST', 'AT']].values\n",
    "X_test_T1 = test_city2_T1[['S', 'A', 'ST', 'AT']].values\n",
    "y_test_T0_pred_mod1 = mod1.predict(X_test_T0)\n",
    "y_test_T1_pred_mod1 = mod1.predict(X_test_T1)\n",
    "\n",
    "X_test_T0 = test_city2_T0[['A', 'AT']].values\n",
    "X_test_T1 = test_city2_T1[['A', 'AT']].values\n",
    "y_test_T0_pred_mod2 = mod2.predict(X_test_T0)\n",
    "y_test_T1_pred_mod2 = mod2.predict(X_test_T1)\n",
    "\n",
    "# In-distribution (city 1) predictions model 1\n",
    "df_city2_mod1 = pd.DataFrame.from_dict({'y_test_T0_pred_mod1': y_test_T0_pred_mod1.squeeze(),\n",
    "                                        'y_test_T1_pred_mod1': y_test_T1_pred_mod1.squeeze(),\n",
    "                                        'y_test_diff_pred_mod1': y_test_T1_pred_mod1.squeeze() - y_test_T0_pred_mod1.squeeze(),\n",
    "                                        'y_test_diff': data_city2['Y1'].values - data_city2['Y0'].values\n",
    "                                       })\n",
    "\n",
    "# In-distribution (city 1) predictions model 2\n",
    "\n",
    "df_city2_mod2 = pd.DataFrame.from_dict({'y_test_T0_pred_mod2': y_test_T0_pred_mod2.squeeze(),\n",
    "                                        'y_test_T1_pred_mod2': y_test_T1_pred_mod2.squeeze(),\n",
    "                                        'y_test_diff_pred_mod2': y_test_T1_pred_mod2.squeeze() - y_test_T0_pred_mod2.squeeze(),\n",
    "                                        'y_test_diff': data_city2['Y1'].values - data_city2['Y0'].values\n",
    "                                        })\n",
    "\n",
    "\n",
    "\n",
    "print(\"MSE - Model 1 - OOD:\", \n",
    "mean_squared_error(df_city2_mod1['y_test_diff'].values, df_city2_mod1['y_test_diff_pred_mod1'].values))\n",
    "\n",
    "print(\"MSE - Model 2 - OOD:\", \n",
    "mean_squared_error(df_city2_mod2['y_test_diff'].values, df_city2_mod2['y_test_diff_pred_mod2'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't observe the true lift `y_test_diff`. Hence, let's mimic which features would be selected using conventional methods for selecting CATE models. Here we use the `RScorer`, which returns an analogue of the R-square score for regression. We see below, than under many alternative CATE methodologies, we will always pick the model that includes Savings. Those, as we've shown before, this model performs very poorly in the new environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection using R Scorer https://github.com/microsoft/EconML/blob/main/notebooks/Causal%20Model%20Selection%20with%20the%20RScorer.ipynb\n",
    "# https://econml.azurewebsites.net/_autosummary/econml.score.RScorer.html\n",
    "# Model 1\n",
    "\n",
    "import econml\n",
    "\n",
    "from econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML\n",
    "from econml.metalearners import XLearner, TLearner, SLearner, DomainAdaptationLearner\n",
    "from econml.dr import DRLearner\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "reg = lambda: RandomForestRegressor(min_samples_leaf=10)\n",
    "clf = lambda: RandomForestClassifier(min_samples_leaf=10)\n",
    "\n",
    "data_city1 = scm_example_4(2000, 1, 42)\n",
    "X = data_city1[['S', 'A']].values\n",
    "T = data_city1[['T']].values.squeeze()\n",
    "Y = data_city1[['Y_obs']].values.squeeze()              \n",
    "               \n",
    "X_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, Y, test_size=.5)\n",
    "\n",
    "\n",
    "models = [('ldml', LinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,\n",
    "                             linear_first_stages=False, cv=3)),\n",
    "          ('sldml', SparseLinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,\n",
    "                                    featurizer=PolynomialFeatures(degree=2, include_bias=False),\n",
    "                                    linear_first_stages=False, cv=3)),\n",
    "          ('xlearner', XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),\n",
    "          ('dalearner', DomainAdaptationLearner(models=reg(), final_models=reg(), propensity_model=clf())),\n",
    "          ('slearner', SLearner(overall_model=reg())),\n",
    "          ('tlearner', TLearner(models=reg())),\n",
    "          ('drlearner', DRLearner(model_propensity=clf(), model_regression=reg(),\n",
    "                                  model_final=reg(), cv=3)),\n",
    "          ('rlearner', NonParamDML(model_y=reg(), model_t=clf(), model_final=reg(),\n",
    "                                   discrete_treatment=True, cv=3)),\n",
    "          ('dml3dlasso', DML(model_y=reg(), model_t=clf(), model_final=LassoCV(), discrete_treatment=True,\n",
    "                             featurizer=PolynomialFeatures(degree=3),\n",
    "                             linear_first_stages=False, cv=3))\n",
    "]\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def fit_model(name, model):\n",
    "    return name, model.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "models = Parallel(n_jobs=-1, verbose=1)(delayed(fit_model)(name, mdl) for name, mdl in models)\n",
    "\n",
    "from econml.score import RScorer\n",
    "\n",
    "scorer = RScorer(model_y=reg(), model_t=clf(),\n",
    "                 discrete_treatment=True, cv=3,\n",
    "                 mc_iters=3, mc_agg='median')\n",
    "scorer.fit(Y_val, T_val, X=X_val)\n",
    "\n",
    "\n",
    "rscore = [scorer.score(mdl) for _, mdl in models]\n",
    "\n",
    "rscore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection using R Scorer https://github.com/microsoft/EconML/blob/main/notebooks/Causal%20Model%20Selection%20with%20the%20RScorer.ipynb\n",
    "# Model 2\n",
    "\n",
    "import econml\n",
    "\n",
    "from econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML\n",
    "from econml.metalearners import XLearner, TLearner, SLearner, DomainAdaptationLearner\n",
    "from econml.dr import DRLearner\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "reg = lambda: RandomForestRegressor(min_samples_leaf=10)\n",
    "clf = lambda: RandomForestClassifier(min_samples_leaf=10)\n",
    "\n",
    "data_city1 = scm_example_4(2000, 1, 42)\n",
    "\n",
    "X = data_city1[['A']].values\n",
    "T = data_city1[['T']].values.squeeze()\n",
    "Y = data_city1[['Y_obs']].values.squeeze()              \n",
    "               \n",
    "X_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, Y, test_size=.5)\n",
    "\n",
    "models = [('ldml', LinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,\n",
    "                             linear_first_stages=False, cv=3)),\n",
    "          ('sldml', SparseLinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,\n",
    "                                    featurizer=PolynomialFeatures(degree=2, include_bias=False),\n",
    "                                    linear_first_stages=False, cv=3)),\n",
    "          ('xlearner', XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),\n",
    "          ('dalearner', DomainAdaptationLearner(models=reg(), final_models=reg(), propensity_model=clf())),\n",
    "          ('slearner', SLearner(overall_model=reg())),\n",
    "          ('tlearner', TLearner(models=reg())),\n",
    "          ('drlearner', DRLearner(model_propensity=clf(), model_regression=reg(),\n",
    "                                  model_final=reg(), cv=3)),\n",
    "          ('rlearner', NonParamDML(model_y=reg(), model_t=clf(), model_final=reg(),\n",
    "                                   discrete_treatment=True, cv=3)),\n",
    "          ('dml3dlasso', DML(model_y=reg(), model_t=clf(), model_final=LassoCV(), discrete_treatment=True,\n",
    "                             featurizer=PolynomialFeatures(degree=3),\n",
    "                             linear_first_stages=False, cv=3))\n",
    "]\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def fit_model(name, model):\n",
    "    return name, model.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "models = Parallel(n_jobs=-1, verbose=1)(delayed(fit_model)(name, mdl) for name, mdl in models)\n",
    "\n",
    "from econml.score import RScorer\n",
    "\n",
    "scorer = RScorer(model_y=reg(), model_t=clf(),\n",
    "                 discrete_treatment=True, cv=3,\n",
    "                 mc_iters=3, mc_agg='median')\n",
    "scorer.fit(Y_val, T_val, X=X_val)\n",
    "\n",
    "\n",
    "rscore = [scorer.score(mdl) for _, mdl in models]\n",
    "\n",
    "rscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import econml\n",
    "\n",
    "from econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML\n",
    "from econml.metalearners import XLearner, TLearner, SLearner, DomainAdaptationLearner\n",
    "from econml.dr import DRLearner\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def scm_example_4(n_samples:int=100, city=None, seed = None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N = np.random.normal(0.0, 1.0, (n_samples, 7))\n",
    "    N_U1, N_U2, N_I, N_A, N_S, N_T, N_Y =  (N[:,0], N[:,1], N[:,2], N[:,3], N[:,4], N[:,5],  N[:,6])\n",
    "    U1 = N_U1\n",
    "    U2 = N_U2\n",
    "    A = N_A\n",
    "    I = 0.5 * U1 +  N_I\n",
    "    \n",
    "    if city==1:\n",
    "        S = 0.5 * I + 0.1 * N_S\n",
    "        T = np.random.binomial(1, 0.5, n_samples)\n",
    "        \n",
    "    elif city ==2:\n",
    "        S = I ** 2 + 0.1 * N_S\n",
    "        T = 0.2 * U1 + 0.2 * U2 + 1.5 * I + 0.3 * N_T\n",
    "    \n",
    "    Y_obs = 1.5 * T + 0.8 * I + 2 * I * T + 0.5 * A + 0.5 * A * T + 0.2 * U2 + 1.5 * N_Y\n",
    "    \n",
    "    Y1 = 1.5 * 1 + 0.8 * I + 2 * I * 1 + 0.5 * A + 0.5 * A * 1 + 0.2 * U2 + 1.5 * N_Y\n",
    "    Y0 = 1.5 * 0 + 0.8 * I + 2 * I * 0 + 0.5 * A + 0.5 * A * 0 + 0.2 * U2 + 1.5 * N_Y\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'I':I, 'S':S, 'A': A, 'T':T, 'ST': S * T, 'AT': A * T, \n",
    "                                 'Y_obs':Y_obs, 'Y1':Y1, 'Y0':Y0})\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def sim(B=100, model=1, seed=42,\n",
    "         #reg = lambda: LinearRegression(),\n",
    "          reg = lambda: RandomForestRegressor(min_samples_leaf=10)\n",
    "       ):\n",
    "    \"\"\"\n",
    "    B = Number of repetitions\n",
    "    model = model 1 includes Savings, model 2 does not\n",
    "    seed = random seed\n",
    "    \n",
    "    \"\"\" \n",
    "    mse_ID = np.empty([B, 3])\n",
    "    mse_OOD = np.empty([B, 3])\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    r = [np.random.randint(1,10000) for _ in range(B)]\n",
    "       \n",
    "    for b in range(B):\n",
    "\n",
    "        data_city1 = scm_example_4(2000, 1, r)\n",
    "        \n",
    "        data_city2 = scm_example_4(2000, 2, r)\n",
    "\n",
    "        train_city1, val_city1 = train_test_split(data_city1, test_size=0.5)\n",
    "        \n",
    "        if model ==1:\n",
    "            X_train = train_city1[['S', 'A']].values\n",
    "            X_val = val_city1[['S', 'A']].values\n",
    "            X_city2 = data_city2[['S', 'A']].values\n",
    "        elif model ==2:\n",
    "            X_train = train_city1[['A']].values\n",
    "            X_val = val_city1[['A']].values\n",
    "            X_city2 = data_city2[['A']].values\n",
    "        T_train = train_city1[['T']].values.squeeze()\n",
    "        Y_train = train_city1[['Y_obs']].values.squeeze()              \n",
    "\n",
    "        models = [('xlearner', XLearner(models=reg())),\n",
    "                  #('slearner', SLearner(overall_model=reg())),\n",
    "                  ('slearner', DomainAdaptationLearner(models=reg(), final_models=reg())),\n",
    "                  ('tlearner', TLearner(models=reg()))]\n",
    "\n",
    "        def fit_model(name, model):\n",
    "            return name, model.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "        models = Parallel(n_jobs=-1, verbose=1)(delayed(fit_model)(name, mdl) for name, mdl in models)\n",
    "\n",
    "        for i in range(len(models)):\n",
    "            cate_pred_ID = models[i][1].effect(X_val)\n",
    "            cate_actual_ID = val_city1[['Y1']].values.squeeze()- val_city1[['Y0']].values.squeeze()   \n",
    "            mse_ID[b,i] = mean_squared_error(cate_actual_ID, cate_pred_ID)\n",
    "\n",
    "\n",
    "            cate_actual_OOD = data_city2[['Y1']].values.squeeze()- data_city2[['Y0']].values.squeeze()   \n",
    "            cate_pred_OOD = models[i][1].effect(X_city2)\n",
    "            mse_OOD[b,i] = mean_squared_error(cate_actual_OOD, cate_pred_OOD)\n",
    "            \n",
    "            \n",
    "    return mse_ID, mse_OOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model1 = sim(B=100, model=1)\n",
    "res_model2 = sim(B=100, model=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model1 - ID\", res_model1[0])\n",
    "print(\"model2 - ID\", res_model2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"model1 - OOD\", res_model1[1])\n",
    "#print(\"model2 - OOD\", res_model2[1])\n",
    "\n",
    "\n",
    "df_ID1 = pd.DataFrame.from_dict({'XLearner':res_model1[0][:,0],\n",
    "                                 'SLearner':res_model1[0][:,1],\n",
    "                                 'TLearner':res_model1[0][:,2], \n",
    "                                 'Model': 1,\n",
    "                               })\n",
    "\n",
    "\n",
    "df_ID2 = pd.DataFrame.from_dict({\n",
    "                                'XLearner':res_model2[0][:,0],\n",
    "                                'SLearner':res_model2[0][:,1],\n",
    "                                'TLearner':res_model2[0][:,2],   \n",
    "                                'Model': 2,\n",
    "                               })\n",
    "\n",
    "\n",
    "df_ID = pd.concat([df_ID1, df_ID2], axis= 0)\n",
    "\n",
    "df_ID = pd.melt(df_ID, id_vars=['Model'], var_name='Learner', value_name='CATE_MSE')\n",
    "\n",
    "\n",
    "df_OOD1 = pd.DataFrame.from_dict({'XLearner':res_model1[1][:,0],\n",
    "                                  'SLearner':res_model1[1][:,1],\n",
    "                                  'TLearner':res_model1[1][:,2],\n",
    "                                  'Model': 1,\n",
    "                               })\n",
    "\n",
    "df_OOD2 = pd.DataFrame.from_dict({'XLearner':res_model2[1][:,0],\n",
    "                                  'SLearner':res_model2[1][:,1],\n",
    "                                  'TLearner':res_model2[1][:,2],\n",
    "                                  'Model': 2,\n",
    "                               })\n",
    "\n",
    "df_OOD = pd.concat([df_OOD1, df_OOD2], axis= 0)\n",
    "\n",
    "df_OOD = pd.melt(df_OOD, id_vars=['Model'], var_name='Learner', value_name='CATE_MSE')\n",
    "\n",
    "\n",
    "\n",
    "fig = px.box(df_ID, x=\"Learner\", y=\"CATE_MSE\", color=\"Model\", labels={'Learner':\"\"})\n",
    "fig.update_traces() # or \"inclusive\", or \"linear\" by default\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(df_OOD, x=\"Learner\", y=\"CATE_MSE\", color=\"Model\", labels={'Learner':\"\"})\n",
    "fig.update_traces() # or \"inclusive\", or \"linear\" by default\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for presentation - assume now Income is observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "import econml\n",
    "\n",
    "from econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML\n",
    "from econml.metalearners import XLearner, TLearner, SLearner, DomainAdaptationLearner\n",
    "from econml.dr import DRLearner\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def scm_example_4_mod(n_samples:int=100, city=None, seed = None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N = np.random.normal(0.0, 1.0, (n_samples, 7))\n",
    "    N_U1, N_U2, N_I, N_A, N_S, N_T, N_Y =  (N[:,0], N[:,1], N[:,2], N[:,3], N[:,4], N[:,5],  N[:,6])\n",
    "    U1 = N_U1\n",
    "    U2 = N_U2\n",
    "    A = N_A\n",
    "    I = 0.5 * U1 +  N_I\n",
    "    \n",
    "    if city==1:\n",
    "        S = 0.5 * I + 0.1 * N_S\n",
    "        T = np.random.binomial(1, 0.5, n_samples)\n",
    "        \n",
    "    elif city ==2:\n",
    "        S = I ** 2 + 0.1 * N_S\n",
    "        T = 0.2 * U1 + 0.2 * U2 + 1.5 * I + 0.3 * N_T\n",
    "    \n",
    "    Y_obs = 1.5 * T + 0.8 * I + 2 * I * T + 0.5 * A + 0.5 * A * T + 0.2 * U2 + 1.5 * N_Y\n",
    "    \n",
    "    Y1 = 1.5 * 1 + 0.8 * I + 2 * I * 1 + 0.5 * A + 0.5 * A * 1 + 0.2 * U2 + 1.5 * N_Y\n",
    "    Y0 = 1.5 * 0 + 0.8 * I + 2 * I * 0 + 0.5 * A + 0.5 * A * 0 + 0.2 * U2 + 1.5 * N_Y\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'I':I, 'S':S, 'A': A, 'T':T, 'ST': S * T, 'AT': A * T, 'IT': I * T, \n",
    "                                 'Y_obs':Y_obs, 'Y1':Y1, 'Y0':Y0})\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def sim(B=100, model=1, seed=42,\n",
    "         #reg = lambda: LinearRegression(),\n",
    "          reg = lambda: RandomForestRegressor(min_samples_leaf=10)\n",
    "       ):\n",
    "    \"\"\"\n",
    "    B = Number of repetitions\n",
    "    model = model 1 includes Savings, model 2 does not\n",
    "    seed = random seed\n",
    "    \n",
    "    \"\"\" \n",
    "    mse_ID = np.empty([B, 3])\n",
    "    mse_OOD = np.empty([B, 3])\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    r = [np.random.randint(1,10000) for _ in range(B)]\n",
    "       \n",
    "    for b in range(B):\n",
    "\n",
    "        data_city1 = scm_example_4_mod(2000, 1, r)\n",
    "        \n",
    "        data_city2 = scm_example_4_mod(2000, 2, r)\n",
    "\n",
    "        train_city1, val_city1 = train_test_split(data_city1, test_size=0.5)\n",
    "        \n",
    "        if model ==1:\n",
    "            X_train = train_city1[['S', 'A', 'I']].values\n",
    "            X_val = val_city1[['S', 'A', 'I']].values\n",
    "            X_city2 = data_city2[['S', 'A', 'I']].values\n",
    "        elif model ==2:\n",
    "            X_train = train_city1[['A']].values\n",
    "            X_val = val_city1[['A']].values\n",
    "            X_city2 = data_city2[['A']].values\n",
    "        T_train = train_city1[['T']].values.squeeze()\n",
    "        Y_train = train_city1[['Y_obs']].values.squeeze()              \n",
    "\n",
    "        models = [('xlearner', XLearner(models=reg())),\n",
    "                  #('slearner', SLearner(overall_model=reg())),\n",
    "                  ('slearner', DomainAdaptationLearner(models=reg(), final_models=reg())),\n",
    "                  ('tlearner', TLearner(models=reg()))]\n",
    "\n",
    "        def fit_model(name, model):\n",
    "            return name, model.fit(Y_train, T_train, X=X_train)\n",
    "\n",
    "        models = Parallel(n_jobs=-1, verbose=1)(delayed(fit_model)(name, mdl) for name, mdl in models)\n",
    "\n",
    "        for i in range(len(models)):\n",
    "            cate_pred_ID = models[i][1].effect(X_val)\n",
    "            cate_actual_ID = val_city1[['Y1']].values.squeeze()- val_city1[['Y0']].values.squeeze()   \n",
    "            mse_ID[b,i] = mean_squared_error(cate_actual_ID, cate_pred_ID)\n",
    "\n",
    "\n",
    "            cate_actual_OOD = data_city2[['Y1']].values.squeeze()- data_city2[['Y0']].values.squeeze()   \n",
    "            cate_pred_OOD = models[i][1].effect(X_city2)\n",
    "            mse_OOD[b,i] = mean_squared_error(cate_actual_OOD, cate_pred_OOD)\n",
    "            \n",
    "            \n",
    "    return mse_ID, mse_OOD\n",
    "\n",
    "res_model1 = sim(B=100, model=1)\n",
    "res_model2 = sim(B=100, model=2)\n",
    "\n",
    "\n",
    "df_ID1 = pd.DataFrame.from_dict({'XLearner':res_model1[0][:,0],\n",
    "                                 'SLearner':res_model1[0][:,1],\n",
    "                                 'TLearner':res_model1[0][:,2], \n",
    "                                 'Model': 1,\n",
    "                               })\n",
    "\n",
    "\n",
    "df_ID2 = pd.DataFrame.from_dict({\n",
    "                                'XLearner':res_model2[0][:,0],\n",
    "                                'SLearner':res_model2[0][:,1],\n",
    "                                'TLearner':res_model2[0][:,2],   \n",
    "                                'Model': 2,\n",
    "                               })\n",
    "\n",
    "df_ID = pd.concat([df_ID1, df_ID2], axis= 0)\n",
    "\n",
    "df_ID = pd.melt(df_ID, id_vars=['Model'], var_name='Learner', value_name='CATE_MSE')\n",
    "\n",
    "\n",
    "df_OOD1 = pd.DataFrame.from_dict({'XLearner':res_model1[1][:,0],\n",
    "                                  'SLearner':res_model1[1][:,1],\n",
    "                                  'TLearner':res_model1[1][:,2],\n",
    "                                  'Model': 1,\n",
    "                               })\n",
    "\n",
    "df_OOD2 = pd.DataFrame.from_dict({'XLearner':res_model2[1][:,0],\n",
    "                                  'SLearner':res_model2[1][:,1],\n",
    "                                  'TLearner':res_model2[1][:,2],\n",
    "                                  'Model': 2,\n",
    "                               })\n",
    "\n",
    "df_OOD = pd.concat([df_OOD1, df_OOD2], axis= 0)\n",
    "\n",
    "df_OOD = pd.melt(df_OOD, id_vars=['Model'], var_name='Learner', value_name='CATE_MSE')\n",
    "\n",
    "\n",
    "fig = px.box(df_ID, x=\"Learner\", y=\"CATE_MSE\", color=\"Model\", labels={'Learner':\"\"})\n",
    "fig.update_traces() # or \"inclusive\", or \"linear\" by default\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(df_OOD, x=\"Learner\", y=\"CATE_MSE\", color=\"Model\", labels={'Learner':\"\"})\n",
    "fig.update_traces() # or \"inclusive\", or \"linear\" by default\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5\n",
    "\n",
    "**Question:** \n",
    "\n",
    "* Is the condition for CATE direct transportability related to $Y$ (actual) or $\\hat{Y}$ (predicted)?\n",
    "* Is the transportability question the same as the question on model robustness?\n",
    "\n",
    "For instance, look at the graph below. \n",
    "* Here we have two paths from $Y$ to $\\delta$ in the graph $G_{\\bar{x}}$: 1) $Y \\leftarrow \\text{Age} \\rightarrow \\text{Savings} \\leftarrow \\delta $, and 2) $Y \\leftarrow \\text{Savings} \\leftarrow \\delta$. We can close the second path by conditioning on $\\text{Savings}$, but that would open the first path since $\\text{Savings}$ is a collider (notice the second path is blocked unless we condition on $\\text{Savings}$. Conditioning on $\\text{Age}$ would close the path again, but this variable is not observed). Note also, that dependence between $Y$ and $\\delta$ implies the model is not directly transportable. \n",
    "\n",
    "So in this example, if the condition for direct transportability is given by $Y$, I cannot achieve direct transportability in this case. However, if it is defined based on $\\hat{Y}$, in this case I can achieve Direct Transportability by only using $\\text{Gender}$, and $T$ (plus their interaction) as a predictors on the experimental data.\n",
    "\n",
    "**Is by definition CATE transportable if $P^*(y|do(t), x)$ is identifyable from $P(y|do(t), x)$ and $p(y, t, x)$ and $p^*(y, t, x)$?** I think so, but what does identifyable mean? It means that it can be computed based on the information available to us. See \"Genralizing Experimental Findings\" (Pearl), Page 260 (Pink highlight).\n",
    "\n",
    "So in this case, although $P^*(y|do(t), x)$ becuase I would need to condition on both $Age$ and $Savings$, but $Age$ is unobserved. However, a model that only considers Gender + T + Gender x T would do better OOD compared to a model that also includes Savings, again since that would open the first path. \n",
    "\n",
    "<img src=\"img/fig7.png\" \n",
    "     width=\"250\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Distribution Generalization of CATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we don't have access to data in the target domain. In this case, the condition for generalizibaility (i.e., to have stable = invariant models to new environments) is to build models such that $(Y \\perp \\!\\!\\! \\perp \\delta | T, X)$ in $G_{\\bar{T}}$ (i.e, yo have models that are exhibit direct transportability). \n",
    "\n",
    " No access to data in target domain. Not having access to target domain data has two impication: 1) I cannot use data from target domain to detect changes in causal mechanisms; we need to inkove expert knowledge, 2) the only hope to get transportability is through direct transportability (as trivial transportability is not feasible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Recipe \n",
    "\n",
    "CATE transportability under 2 scenarios (access to data in target domain $\\pi^*$ or no access to such data).\n",
    "\n",
    "<img src=\"img/recipe.png\" \n",
    "     width=\"350\"\n",
    "     alt=\"Title\"\n",
    "     title=\"Title\"\n",
    "     >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection bias in CATE estimation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "The key inights from these examples is that generalization of CATE is not just a statistical problem, it is a causal one. That is the generalization depends on (i) the causal dag, and (ii) the specific causal mechanisms that change between source and target populations, (iii) admissibility assumption?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Changes in Causal Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If data are avaible in the target environment \n",
    "* If data are not available need to rely on expert judgment\n",
    "\n",
    "* See Section 5 from \"why did the distribution change\"? Also see Section 7 practical example (page 16 in particular). For root node it uses the empirical distribution, for non-root node fits xgboost. \n",
    "\n",
    "**Use Kernel two-sample tests**\n",
    "https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf\n",
    "https://www.tandfonline.com/doi/full/10.1080/10618600.2022.2067547\n",
    "https://torchdrift.org/notebooks/note_on_mmd.html\n",
    "\n",
    "**Use non-parametric (kernel-based) conditional indpendence tests.** (Zhang et al., 2011)\n",
    "* https://pypi.org/project/PyRKHSstats/\n",
    "* https://github.com/Black-Swan-ICL/PyRKHSstats\n",
    "\n",
    "* https://causal-learn.readthedocs.io/en/latest/independence_tests_index/kci.html\n",
    "\n",
    "* https://rdrr.io/cran/CondIndTests/man/KCI.html\n",
    "* https://cran.r-project.org/web/packages/CondIndTests/index.html\n",
    "https://cran.r-project.org/web/packages/kpcalg/kpcalg.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "import tqdm\n",
    "\n",
    "def _mmd(x, y, sigma):\n",
    "    # compare kernel MMD paper and code:\n",
    "    # A. Gretton et al.: A kernel two-sample test, JMLR 13 (2012)\n",
    "    # http://www.gatsby.ucl.ac.uk/~gretton/mmd/mmd.htm\n",
    "    # x shape [n, d] y shape [m, d]\n",
    "    n, d = x.shape\n",
    "    m, d2 = y.shape\n",
    "    assert d == d2\n",
    "    xy = torch.cat([x.detach(), y.detach()], dim=0)\n",
    "    dists = torch.cdist(xy, xy, p=2.0)\n",
    "    # we are a bit sloppy here as we just keep the diagonal and everything twice\n",
    "    # note that sigma should be squared in the RBF to match the Gretton et al heuristic\n",
    "    k = torch.exp((-1/(2*sigma**2)) * dists**2) + torch.eye(n+m)*1e-5\n",
    "    k_x = k[:n, :n]\n",
    "    k_y = k[n:, n:]\n",
    "    k_xy = k[:n, n:]\n",
    "    # The diagonals are always 1 (up to numerical error, this is (3) in Gretton et al.)\n",
    "    # note that their code uses the biased (and differently scaled mmd)\n",
    "    mmd = k_x.sum() / (n * (n - 1)) + k_y.sum() / (m * (m - 1)) - 2 * k_xy.sum() / (n * m)\n",
    "    return mmd\n",
    "\n",
    "def mmd_test(x, y, B=999):\n",
    "    \"\"\"\n",
    "    B = number of bootstrap permutations to get p-value, pass none to not get p-value\n",
    "    \"\"\"\n",
    "    dists = torch.pdist(torch.cat([x, y], dim=0)[:,None])\n",
    "    sigma = dists.median()/2\n",
    "    mmd_obs = _mmd(x[:, None], y[:, None], sigma)\n",
    "\n",
    "    N_X = len(x)\n",
    "    N_Y = len(y)\n",
    "    xy = torch.cat([x, y], dim=0)[:, None].double()\n",
    "\n",
    "    mmds = []\n",
    "    for i in tqdm.tqdm(range(B)):\n",
    "        xy = xy[torch.randperm(len(xy))]\n",
    "        mmds.append(_mmd(xy[:N_X], xy[N_X:], sigma).item())\n",
    "    mmds = torch.tensor(mmds)\n",
    "    \n",
    "    #mmds = Parallel(n_jobs=-1, verbose=1)(delayed(_mmd)(xy[torch.randperm(len(xy))][:N_X], xy[torch.randperm(len(xy))][N_X:], sigma) for i in range(B))\n",
    "    #mmds = torch.tensor(mmds)\n",
    "    \n",
    "    p_value = (mmd_obs < mmds).float().mean()\n",
    "    \n",
    "    return p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_city1 = scm_example_2(200, 1, 42)\n",
    "data_city2 = scm_example_2(200, 2, 83)\n",
    "\n",
    "print(\"Variable:I - MMD P-value\", \n",
    "    mmd_test(torch.from_numpy(data_city1['I'].values), torch.from_numpy(data_city2['I'].values), B=999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#model1 = xgb.XGBRegressor()\n",
    "model1 = LinearRegression()\n",
    "model1.fit(data_city1[['T_', 'I']].values , data_city1['Y_obs'].values)\n",
    "y_pred1 = model1.predict(data_city1[['T_', 'I']].values)\n",
    "\n",
    "#model2 = xgb.XGBRegressor()\n",
    "model2 = LinearRegression()\n",
    "model2.fit(data_city2[['T', 'I']].values , data_city2['Y_obs'].values)\n",
    "y_pred2 = model2.predict(data_city2[['T', 'I']].values)\n",
    "\n",
    "print(\"Variable:Y|pa(Y) - MMD P-value\", \n",
    "    mmd_test(torch.from_numpy(y_pred1), torch.from_numpy(y_pred2), B=999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 - data_city1['Y_obs'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Discovery \n",
    "\n",
    "In all these examples, we assume we are equipped with a causal model (graph), that encodes the causal relationships in $\\pi$ and $\\pi^*$.\n",
    "\n",
    "I assume graph is given in all examples, need to use methods for causal discovery. Good thing is that I'm assuming I have experiments conducted in source population, which should facilitate causal discovery. \n",
    "\n",
    "* See Brady videos on Causal Discovery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate end-to-end problem \n",
    "\n",
    "Using causal dicovery, mechanism changes detection, and right adjustment forumula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
